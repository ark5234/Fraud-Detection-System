{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cab9108",
   "metadata": {},
   "source": [
    "# Fraud Detection Case Study\n",
    "This notebook presents a complete workflow for proactive fraud detection in financial transactions, including data cleaning, model development, performance evaluation, and actionable recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e45d5e",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition & Dictionary\n",
    "- Data source: [Link to dataset]\n",
    "- Data dictionary: [Link to data dictionary]\n",
    "\n",
    "*Please download the CSV file and place it in the workspace before running the notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b7c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b34d4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:15:57) [MSC v.1916 64 bit (AMD64)]\n",
      "pandas: 2.3.1\n",
      "numpy: 1.26.4\n",
      "scikit-learn: 1.7.1\n",
      "matplotlib: 3.10.5\n",
      "seaborn: 0.13.2\n",
      "statsmodels: 0.14.5\n"
     ]
    }
   ],
   "source": [
    "# Runtime check: Python and package versions\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import statsmodels\n",
    "print('Python:', sys.version)\n",
    "print('pandas:', pd.__version__)\n",
    "print('numpy:', np.__version__)\n",
    "print('scikit-learn:', sklearn.__version__)\n",
    "print('matplotlib:', matplotlib.__version__)\n",
    "print('seaborn:', sns.__version__)\n",
    "print('statsmodels:', statsmodels.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a93006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages OK\n"
     ]
    }
   ],
   "source": [
    "# Ensure required packages in this kernel\n",
    "try:\n",
    "    import pandas as pd, numpy as np, matplotlib, seaborn as sns, sklearn, statsmodels\n",
    "    print('Packages OK')\n",
    "except ModuleNotFoundError as e:\n",
    "    import sys, subprocess\n",
    "    pkgs = ['pandas','numpy','matplotlib','seaborn','scikit-learn','statsmodels']\n",
    "    print(f'Installing missing: {e.name} ...')\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', *pkgs])\n",
    "    print('Packages installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6574537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = ''              # Set to CSV path or leave blank for autodetect\n",
    "USE_SAMPLE = False          # False for full 6.36M rows\n",
    "SAMPLE_SIZE = 1_000_000     # Ignored when USE_SAMPLE=False\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.30\n",
    "DISPLAY_EPS = 1e-12         # Floor for printing tiny probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106801b1",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "We apply scalable, robust cleaning to handle 6.36M rows:\n",
    "\n",
    "- Missing values: Summarized globally; numeric imputed with median in pipeline; categoricals handled by OneHotEncoder with `handle_unknown='ignore'`.\n",
    "\n",
    "- Outliers: IQR-based fences computed; extreme tails clipped in VIF step only to stabilize multi-collinearity estimation (modeling uses raw but log-transformed `amount`).\n",
    "\n",
    "- Multicollinearity: VIF computed on numeric predictors excluding target/flags; used for diagnostics (drop if VIF >> 10). No leakage features used.\n",
    "\n",
    "- High-cardinality IDs (`nameOrig`, `nameDest`) are excluded; instead, we derive low-cardinality signals like `dest_is_merchant`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b9ea976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CSV: c:\\Users\\Vikra\\OneDrive\\Desktop\\Accredian\\fraud.csv\n",
      "(6362620, 11)\n",
      "(6362620, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isFlaggedFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>9839.639648</td>\n",
       "      <td>C1231006815</td>\n",
       "      <td>170136.0</td>\n",
       "      <td>160296.359375</td>\n",
       "      <td>M1979787155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>1864.280029</td>\n",
       "      <td>C1666544295</td>\n",
       "      <td>21249.0</td>\n",
       "      <td>19384.720703</td>\n",
       "      <td>M2044282225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>C1305486145</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>C553264065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>C840083671</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>C38997010</td>\n",
       "      <td>21182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>11668.139648</td>\n",
       "      <td>C2048537720</td>\n",
       "      <td>41554.0</td>\n",
       "      <td>29885.859375</td>\n",
       "      <td>M1230701703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step      type        amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
       "0     1   PAYMENT   9839.639648  C1231006815       170136.0   160296.359375   \n",
       "1     1   PAYMENT   1864.280029  C1666544295        21249.0    19384.720703   \n",
       "2     1  TRANSFER    181.000000  C1305486145          181.0        0.000000   \n",
       "3     1  CASH_OUT    181.000000   C840083671          181.0        0.000000   \n",
       "4     1   PAYMENT  11668.139648  C2048537720        41554.0    29885.859375   \n",
       "\n",
       "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
       "0  M1979787155             0.0             0.0        0               0  \n",
       "1  M2044282225             0.0             0.0        0               0  \n",
       "2   C553264065             0.0             0.0        1               0  \n",
       "3    C38997010         21182.0             0.0        1               0  \n",
       "4  M1230701703             0.0             0.0        0               0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data load\n",
    "from pathlib import Path\n",
    "\n",
    "def _autodetect_csv():\n",
    "    cands = []\n",
    "    if DATA_PATH:\n",
    "        cands.append(Path(DATA_PATH))\n",
    "    here = Path.cwd()\n",
    "    cands += [here / n for n in ['transactions.csv','fraud.csv','fraudDetection.csv','data.csv']]\n",
    "    dld = Path.home() / 'Downloads'\n",
    "    cands += [dld / n for n in ['transactions.csv','PS_20174392719_1491204439457_log.csv','fraud.csv','data.csv']]\n",
    "    for p in cands:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "csv_path = _autodetect_csv()\n",
    "assert csv_path is not None, 'CSV not found. Set DATA_PATH or place the CSV in this folder or Downloads.'\n",
    "print(f'Using CSV: {csv_path}')\n",
    "\n",
    "dtype_map = {\n",
    "    'step': 'int32','type': 'category','amount': 'float32','nameOrig': 'category',\n",
    "    'oldbalanceOrg': 'float32','newbalanceOrig': 'float32','nameDest': 'category',\n",
    "    'oldbalanceDest': 'float32','newbalanceDest': 'float32','isFraud': 'int8','isFlaggedFraud': 'int8',\n",
    "}\n",
    "\n",
    "def read_csv_scalable(path: str, use_sample=True, sample_size=1_000_000, random_state=42):\n",
    "    chunks, total = [], 0\n",
    "    for ch in pd.read_csv(path, dtype=dtype_map, chunksize=200_000):\n",
    "        chunks.append(ch)\n",
    "        total += len(ch)\n",
    "        if use_sample and total >= sample_size:\n",
    "            break\n",
    "    df_ = pd.concat(chunks, ignore_index=True)\n",
    "    if use_sample and len(df_) > sample_size:\n",
    "        df_ = df_.sample(sample_size, random_state=random_state)\n",
    "    df_.columns = [c.strip() for c in df_.columns]\n",
    "    return df_\n",
    "\n",
    "df = read_csv_scalable(str(csv_path), use_sample=USE_SAMPLE, sample_size=SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a8f35fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>...</th>\n",
       "      <th>is_TRANSFER</th>\n",
       "      <th>is_CASH_OUT</th>\n",
       "      <th>amt_log</th>\n",
       "      <th>is_high_value</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>dest_is_merchant</th>\n",
       "      <th>orig_went_zero</th>\n",
       "      <th>dest_went_zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>9839.639648</td>\n",
       "      <td>C1231006815</td>\n",
       "      <td>170136.0</td>\n",
       "      <td>160296.359375</td>\n",
       "      <td>M1979787155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.194276</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>1864.280029</td>\n",
       "      <td>C1666544295</td>\n",
       "      <td>21249.0</td>\n",
       "      <td>19384.720703</td>\n",
       "      <td>M2044282225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.531167</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>C1305486145</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>C553264065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.204007</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>C840083671</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>C38997010</td>\n",
       "      <td>21182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.204007</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>11668.139648</td>\n",
       "      <td>C2048537720</td>\n",
       "      <td>41554.0</td>\n",
       "      <td>29885.859375</td>\n",
       "      <td>M1230701703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.364703</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   step      type        amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
       "0     1   PAYMENT   9839.639648  C1231006815       170136.0   160296.359375   \n",
       "1     1   PAYMENT   1864.280029  C1666544295        21249.0    19384.720703   \n",
       "2     1  TRANSFER    181.000000  C1305486145          181.0        0.000000   \n",
       "3     1  CASH_OUT    181.000000   C840083671          181.0        0.000000   \n",
       "4     1   PAYMENT  11668.139648  C2048537720        41554.0    29885.859375   \n",
       "\n",
       "      nameDest  oldbalanceDest  newbalanceDest  isFraud  ...  is_TRANSFER  \\\n",
       "0  M1979787155             0.0             0.0        0  ...            0   \n",
       "1  M2044282225             0.0             0.0        0  ...            0   \n",
       "2   C553264065             0.0             0.0        1  ...            1   \n",
       "3    C38997010         21182.0             0.0        1  ...            0   \n",
       "4  M1230701703             0.0             0.0        0  ...            0   \n",
       "\n",
       "   is_CASH_OUT   amt_log  is_high_value  hour  day  is_weekend  \\\n",
       "0            0  9.194276              0     1    0           0   \n",
       "1            0  7.531167              0     1    0           0   \n",
       "2            0  5.204007              0     1    0           0   \n",
       "3            1  5.204007              0     1    0           0   \n",
       "4            0  9.364703              0     1    0           0   \n",
       "\n",
       "   dest_is_merchant  orig_went_zero  dest_went_zero  \n",
       "0                 1               0               0  \n",
       "1                 1               0               0  \n",
       "2                 0               1               0  \n",
       "3                 0               1               1  \n",
       "4                 1               0               0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature engineering\n",
    "if 'type' in df.columns:\n",
    "    df['type'] = df['type'].astype('string').str.replace('-', '_').astype('category')\n",
    "\n",
    "df['orig_error'] = (df['newbalanceOrig'] + df['amount'] - df['oldbalanceOrg']).astype('float32')\n",
    "df['dest_error'] = (df['oldbalanceDest'] + df['amount'] - df['newbalanceDest']).astype('float32')\n",
    "\n",
    "df['is_TRANSFER'] = (df['type'] == 'TRANSFER').astype('int8')\n",
    "df['is_CASH_OUT'] = (df['type'] == 'CASH_OUT').astype('int8')\n",
    "\n",
    "df['amt_log'] = np.log1p(df['amount']).astype('float32')\n",
    "df['is_high_value'] = (df['amount'] >= 200_000).astype('int8')\n",
    "\n",
    "df['hour'] = (df['step'] % 24).astype('int8')\n",
    "df['day'] = (df['step'] // 24).astype('int16')\n",
    "df['is_weekend'] = (df['day'] % 7 >= 5).astype('int8')\n",
    "\n",
    "df['dest_is_merchant'] = df['nameDest'].astype('string').str.startswith('M').fillna(False).astype('int8')\n",
    "\n",
    "df['orig_went_zero'] = ((df['newbalanceOrig'] == 0) & (df['oldbalanceOrg'] > 0)).astype('int8')\n",
    "df['dest_went_zero'] = ((df['newbalanceDest'] == 0) & (df['oldbalanceDest'] > 0)).astype('int8')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fce19a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing ratio (%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "step                0.0\n",
       "dest_error          0.0\n",
       "orig_went_zero      0.0\n",
       "dest_is_merchant    0.0\n",
       "is_weekend          0.0\n",
       "day                 0.0\n",
       "hour                0.0\n",
       "is_high_value       0.0\n",
       "amt_log             0.0\n",
       "is_CASH_OUT         0.0\n",
       "is_TRANSFER         0.0\n",
       "orig_error          0.0\n",
       "type                0.0\n",
       "isFlaggedFraud      0.0\n",
       "isFraud             0.0\n",
       "newbalanceDest      0.0\n",
       "oldbalanceDest      0.0\n",
       "nameDest            0.0\n",
       "newbalanceOrig      0.0\n",
       "oldbalanceOrg       0.0\n",
       "nameOrig            0.0\n",
       "amount              0.0\n",
       "dest_went_zero      0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Missingness summary\n",
    "display(df.isna().sum().sort_values(ascending=False).pipe(lambda s: s[s>0]))\n",
    "print('\\nMissing ratio (%)')\n",
    "display((df.isna().mean()*100).round(3).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "572d9eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>95%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "      <th>iqr</th>\n",
       "      <th>lower_fence</th>\n",
       "      <th>upper_fence</th>\n",
       "      <th>outliers_low</th>\n",
       "      <th>outliers_high</th>\n",
       "      <th>outliers_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>orig_went_zero</th>\n",
       "      <td>6362620.0</td>\n",
       "      <td>2.389866e-01</td>\n",
       "      <td>4.264646e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>1520581</td>\n",
       "      <td>1520581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dest_error</th>\n",
       "      <td>6362620.0</td>\n",
       "      <td>5.556721e+04</td>\n",
       "      <td>4.415288e+05</td>\n",
       "      <td>-75885720.0</td>\n",
       "      <td>-353401.667500</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3500.489990</td>\n",
       "      <td>2.935302e+04</td>\n",
       "      <td>4.567410e+05</td>\n",
       "      <td>7.931617e+05</td>\n",
       "      <td>13191234.0</td>\n",
       "      <td>2.935302e+04</td>\n",
       "      <td>-4.402953e+04</td>\n",
       "      <td>7.338254e+04</td>\n",
       "      <td>177957</td>\n",
       "      <td>1256756</td>\n",
       "      <td>1434713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <td>6362620.0</td>\n",
       "      <td>8.338834e+05</td>\n",
       "      <td>2.888242e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14208.000000</td>\n",
       "      <td>1.073152e+05</td>\n",
       "      <td>5.823702e+06</td>\n",
       "      <td>1.602726e+07</td>\n",
       "      <td>59585040.0</td>\n",
       "      <td>1.073152e+05</td>\n",
       "      <td>-1.609728e+05</td>\n",
       "      <td>2.682879e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>1112507</td>\n",
       "      <td>1112507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <td>6362620.0</td>\n",
       "      <td>8.551137e+05</td>\n",
       "      <td>2.924048e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.442584e+05</td>\n",
       "      <td>5.980262e+06</td>\n",
       "      <td>1.617616e+07</td>\n",
       "      <td>49585040.0</td>\n",
       "      <td>1.442584e+05</td>\n",
       "      <td>-2.163876e+05</td>\n",
       "      <td>3.606460e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>1053391</td>\n",
       "      <td>1053391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <td>6362620.0</td>\n",
       "      <td>1.100702e+06</td>\n",
       "      <td>3.399180e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>132705.664062</td>\n",
       "      <td>9.430367e+05</td>\n",
       "      <td>5.147230e+06</td>\n",
       "      <td>1.237182e+07</td>\n",
       "      <td>356015904.0</td>\n",
       "      <td>9.430367e+05</td>\n",
       "      <td>-1.414555e+06</td>\n",
       "      <td>2.357592e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>786135</td>\n",
       "      <td>786135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newbalanceDest</th>\n",
       "      <td>6362620.0</td>\n",
       "      <td>1.224997e+06</td>\n",
       "      <td>3.674129e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>214661.445312</td>\n",
       "      <td>1.111909e+06</td>\n",
       "      <td>5.515716e+06</td>\n",
       "      <td>1.313787e+07</td>\n",
       "      <td>356179264.0</td>\n",
       "      <td>1.111909e+06</td>\n",
       "      <td>-1.667864e+06</td>\n",
       "      <td>2.779773e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>738527</td>\n",
       "      <td>738527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_TRANSFER</th>\n",
       "      <td>6362620.0</td>\n",
       "      <td>8.375622e-02</td>\n",
       "      <td>2.770219e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>532909</td>\n",
       "      <td>532909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig_error</th>\n",
       "      <td>6362620.0</td>\n",
       "      <td>2.010926e+05</td>\n",
       "      <td>6.066504e+05</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2954.197449</td>\n",
       "      <td>68677.253906</td>\n",
       "      <td>2.496411e+05</td>\n",
       "      <td>7.007165e+05</td>\n",
       "      <td>1.559495e+06</td>\n",
       "      <td>92445520.0</td>\n",
       "      <td>2.466869e+05</td>\n",
       "      <td>-3.670762e+05</td>\n",
       "      <td>6.196715e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>408489</td>\n",
       "      <td>408489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amount</th>\n",
       "      <td>6362620.0</td>\n",
       "      <td>1.798619e+05</td>\n",
       "      <td>6.038582e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>449.467593</td>\n",
       "      <td>2224.099597</td>\n",
       "      <td>13389.570312</td>\n",
       "      <td>74871.937500</td>\n",
       "      <td>2.087215e+05</td>\n",
       "      <td>5.186342e+05</td>\n",
       "      <td>1.615980e+06</td>\n",
       "      <td>92445520.0</td>\n",
       "      <td>1.953319e+05</td>\n",
       "      <td>-2.796083e+05</td>\n",
       "      <td>5.017193e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>338078</td>\n",
       "      <td>338078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>6362620.0</td>\n",
       "      <td>9.503158e+00</td>\n",
       "      <td>5.922111e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>-4.500000e+00</td>\n",
       "      <td>2.350000e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>162303</td>\n",
       "      <td>162303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count          mean           std         min  \\\n",
       "orig_went_zero  6362620.0  2.389866e-01  4.264646e-01         0.0   \n",
       "dest_error      6362620.0  5.556721e+04  4.415288e+05 -75885720.0   \n",
       "oldbalanceOrg   6362620.0  8.338834e+05  2.888242e+06         0.0   \n",
       "newbalanceOrig  6362620.0  8.551137e+05  2.924048e+06         0.0   \n",
       "oldbalanceDest  6362620.0  1.100702e+06  3.399180e+06         0.0   \n",
       "newbalanceDest  6362620.0  1.224997e+06  3.674129e+06         0.0   \n",
       "is_TRANSFER     6362620.0  8.375622e-02  2.770219e-01         0.0   \n",
       "orig_error      6362620.0  2.010926e+05  6.066504e+05        -4.0   \n",
       "amount          6362620.0  1.798619e+05  6.038582e+05         0.0   \n",
       "day             6362620.0  9.503158e+00  5.922111e+00         0.0   \n",
       "\n",
       "                           1%           5%           25%            50%  \\\n",
       "orig_went_zero       0.000000     0.000000      0.000000       0.000000   \n",
       "dest_error     -353401.667500    -0.125000      0.000000    3500.489990   \n",
       "oldbalanceOrg        0.000000     0.000000      0.000000   14208.000000   \n",
       "newbalanceOrig       0.000000     0.000000      0.000000       0.000000   \n",
       "oldbalanceDest       0.000000     0.000000      0.000000  132705.664062   \n",
       "newbalanceDest       0.000000     0.000000      0.000000  214661.445312   \n",
       "is_TRANSFER          0.000000     0.000000      0.000000       0.000000   \n",
       "orig_error          -0.007812     0.000000   2954.197449   68677.253906   \n",
       "amount             449.467593  2224.099597  13389.570312   74871.937500   \n",
       "day                  0.000000     0.000000      6.000000       9.000000   \n",
       "\n",
       "                         75%           95%           99%          max  \\\n",
       "orig_went_zero  0.000000e+00  1.000000e+00  1.000000e+00          1.0   \n",
       "dest_error      2.935302e+04  4.567410e+05  7.931617e+05   13191234.0   \n",
       "oldbalanceOrg   1.073152e+05  5.823702e+06  1.602726e+07   59585040.0   \n",
       "newbalanceOrig  1.442584e+05  5.980262e+06  1.617616e+07   49585040.0   \n",
       "oldbalanceDest  9.430367e+05  5.147230e+06  1.237182e+07  356015904.0   \n",
       "newbalanceDest  1.111909e+06  5.515716e+06  1.313787e+07  356179264.0   \n",
       "is_TRANSFER     0.000000e+00  1.000000e+00  1.000000e+00          1.0   \n",
       "orig_error      2.496411e+05  7.007165e+05  1.559495e+06   92445520.0   \n",
       "amount          2.087215e+05  5.186342e+05  1.615980e+06   92445520.0   \n",
       "day             1.300000e+01  2.000000e+01  2.800000e+01         30.0   \n",
       "\n",
       "                         iqr   lower_fence   upper_fence  outliers_low  \\\n",
       "orig_went_zero  0.000000e+00  0.000000e+00  0.000000e+00             0   \n",
       "dest_error      2.935302e+04 -4.402953e+04  7.338254e+04        177957   \n",
       "oldbalanceOrg   1.073152e+05 -1.609728e+05  2.682879e+05             0   \n",
       "newbalanceOrig  1.442584e+05 -2.163876e+05  3.606460e+05             0   \n",
       "oldbalanceDest  9.430367e+05 -1.414555e+06  2.357592e+06             0   \n",
       "newbalanceDest  1.111909e+06 -1.667864e+06  2.779773e+06             0   \n",
       "is_TRANSFER     0.000000e+00  0.000000e+00  0.000000e+00             0   \n",
       "orig_error      2.466869e+05 -3.670762e+05  6.196715e+05             0   \n",
       "amount          1.953319e+05 -2.796083e+05  5.017193e+05             0   \n",
       "day             7.000000e+00 -4.500000e+00  2.350000e+01             0   \n",
       "\n",
       "                outliers_high  outliers_total  \n",
       "orig_went_zero        1520581         1520581  \n",
       "dest_error            1256756         1434713  \n",
       "oldbalanceOrg         1112507         1112507  \n",
       "newbalanceOrig        1053391         1053391  \n",
       "oldbalanceDest         786135          786135  \n",
       "newbalanceDest         738527          738527  \n",
       "is_TRANSFER            532909          532909  \n",
       "orig_error             408489          408489  \n",
       "amount                 338078          338078  \n",
       "day                    162303          162303  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Outliers (IQR)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if not numeric_cols:\n",
    "    print('No numeric columns for outlier analysis')\n",
    "else:\n",
    "    desc = df[numeric_cols].describe(percentiles=[0.01,0.05,0.25,0.5,0.75,0.95,0.99]).T\n",
    "    q = df[numeric_cols].quantile([0.25,0.75])\n",
    "    iqr = q.loc[0.75] - q.loc[0.25]\n",
    "    lf, uf = q.loc[0.25] - 1.5*iqr, q.loc[0.75] + 1.5*iqr\n",
    "    desc['iqr'] = iqr; desc['lower_fence'] = lf; desc['upper_fence'] = uf\n",
    "    out_low = (df[numeric_cols] < lf).sum(); out_high = (df[numeric_cols] > uf).sum()\n",
    "    desc['outliers_low'] = out_low; desc['outliers_high'] = out_high\n",
    "    desc['outliers_total'] = desc['outliers_low'] + desc['outliers_high']\n",
    "    display(desc.sort_values('outliers_total', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af752b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>step</td>\n",
       "      <td>31441.859182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>day</td>\n",
       "      <td>31354.294617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>newbalanceOrig</td>\n",
       "      <td>1357.102289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oldbalanceOrg</td>\n",
       "      <td>1315.951968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>newbalanceDest</td>\n",
       "      <td>72.609239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oldbalanceDest</td>\n",
       "      <td>67.041044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>orig_error</td>\n",
       "      <td>46.283285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amount</td>\n",
       "      <td>40.465224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hour</td>\n",
       "      <td>29.207880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>is_CASH_OUT</td>\n",
       "      <td>4.484572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature           VIF\n",
       "0             step  31441.859182\n",
       "13             day  31354.294617\n",
       "3   newbalanceOrig   1357.102289\n",
       "2    oldbalanceOrg   1315.951968\n",
       "5   newbalanceDest     72.609239\n",
       "4   oldbalanceDest     67.041044\n",
       "6       orig_error     46.283285\n",
       "1           amount     40.465224\n",
       "12            hour     29.207880\n",
       "9      is_CASH_OUT      4.484572"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VIF (numeric only)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "exclude_cols = {'isFraud','isFlaggedFraud'}\n",
    "vif_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
    "X_num = df[vif_cols].copy().fillna(df[vif_cols].median())\n",
    "for c in X_num.columns:\n",
    "    q1, q99 = np.percentile(X_num[c], [1,99]); X_num[c] = X_num[c].clip(q1, q99)\n",
    "X_s = StandardScaler().fit_transform(X_num)\n",
    "vif_data = pd.DataFrame({'feature': X_num.columns,\n",
    "                         'VIF': [variance_inflation_factor(X_s, i) for i in range(X_s.shape[1])]})\n",
    "display(vif_data.sort_values('VIF', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8197a02e",
   "metadata": {},
   "source": [
    "## 3. Model Development\n",
    "\n",
    "We build an interpretable baseline Logistic Regression with class weighting and a full preprocessing pipeline (median imputation, scaling for numeric, one-hot for categorical). This offers:\n",
    "\n",
    "- Speed and scalability on millions of rows\n",
    "\n",
    "- Probabilistic outputs to tune thresholds by business cost (recall vs precision)\n",
    "\n",
    "- Coefficients for explainability. We later recommend evaluating tree-based models (XGBoost/LightGBM) for potential lift.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5536147d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isFraud\n",
       "0    0.998709\n",
       "1    0.001291\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split\n",
    "TARGET = 'isFraud'\n",
    "drop_cols = ['nameOrig','nameDest']\n",
    "features = [c for c in df.columns if c not in drop_cols + [TARGET]]\n",
    "X, y = df[features], df[TARGET].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "y_train.value_counts(normalize=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a9e40b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE)\n\u001b[0;32m     37\u001b[0m search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipe, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_precision\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39mcv, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m clf \u001b[38;5;241m=\u001b[39m search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest params:\u001b[39m\u001b[38;5;124m'\u001b[39m, search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\Vikra\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Vikra\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1046\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1047\u001b[0m     )\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1051\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Vikra\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1605\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vikra\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    993\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    994\u001b[0m         )\n\u001b[0;32m    995\u001b[0m     )\n\u001b[1;32m--> 997\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Vikra\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     75\u001b[0m     (\n\u001b[0;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vikra\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vikra\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vikra\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1799\u001b[0m     ):\n\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Pipeline + tuning (Logistic Regression)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "cat_cols = X_train.select_dtypes(include=['category','object','string']).columns.tolist()\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "def _ohe_dense():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler(with_mean=True))\n",
    "        ]), num_cols),\n",
    "        ('cat', _ohe_dense(), cat_cols)\n",
    "    ], remainder='drop')\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('model', LogisticRegression(max_iter=2000, class_weight='balanced', solver='lbfgs'))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__C': [0.1, 0.5, 1.0, 2.0],\n",
    "    'model__penalty': ['l2'],\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "search = GridSearchCV(pipe, param_grid=param_grid, scoring='average_precision', cv=cv, n_jobs=-1, verbose=0)\n",
    "search.fit(X_train, y_train)\n",
    "clf = search.best_estimator_\n",
    "print('Best params:', search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b2716",
   "metadata": {},
   "source": [
    "## 4. Variable Selection\n",
    "\n",
    "Selection combines diagnostics and domain knowledge:\n",
    "\n",
    "- Drop identifiers/leakage (`nameOrig`, `nameDest`); prefer derived flags (`dest_is_merchant`).\n",
    "\n",
    "- Keep fraud-relevant signals: `type`, `amt_log`, `is_TRANSFER`, `is_CASH_OUT`, `orig_error`, `dest_error`, time-of-day/day-of-week.\n",
    "\n",
    "- Use VIF to spot redundant numeric features; if VIF >> 10 and no clear added value, drop the higher-variance one.\n",
    "\n",
    "- Validate choices via cross-validated PR AUC; retain features that improve recall at acceptable precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients (importance)\n",
    "model = clf.named_steps['model']\n",
    "prep = clf.named_steps['prep']\n",
    "ohe = prep.named_transformers_['cat']\n",
    "cat_names = ohe.get_feature_names_out(cat_cols) if len(cat_cols) else np.array([])\n",
    "num_names = np.array(num_cols)\n",
    "feat_names = np.concatenate([num_names, cat_names])\n",
    "coefs = pd.Series(model.coef_.ravel(), index=feat_names).sort_values(key=np.abs, ascending=False)\n",
    "print(coefs.head(20))\n",
    "coefs.head(20).plot(kind='bar', figsize=(10,4))\n",
    "plt.title('Top coefficients'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a31f3f",
   "metadata": {},
   "source": [
    "## 5. Model Performance\n",
    "\n",
    "We report:\n",
    "\n",
    "- ROC AUC (ranking quality) and PR AUC (more informative for class imbalance)\n",
    "\n",
    "- Threshold tuned by F2 (recall emphasis) with classification report and confusion matrix\n",
    "\n",
    "- ROC and PR curves for visual assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3434a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, roc_curve,\n",
    "                             average_precision_score, precision_recall_curve)\n",
    "\n",
    "proba = clf.predict_proba(X_test)[:,1]\n",
    "proba_print = np.maximum(proba, DISPLAY_EPS)  # avoid printing 0.0\n",
    "roc_auc = roc_auc_score(y_test, proba)\n",
    "pr_auc = average_precision_score(y_test, proba)\n",
    "print(f'ROC AUC: {roc_auc:.4f} | PR AUC: {pr_auc:.4f}')\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, proba)\n",
    "f2 = (5*precision*recall)/(4*precision+recall+1e-12)\n",
    "best_idx = f2.argmax(); best_thr = thresholds[max(0, best_idx-1)] if len(thresholds) else 0.5\n",
    "print(f'Best threshold by F2: {best_thr:.6f}')\n",
    "\n",
    "y_pred_thr = (proba >= best_thr).astype(int)\n",
    "print(classification_report(y_test, y_pred_thr, digits=4))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred_thr))\n",
    "\n",
    "# Curves\n",
    "fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "plt.figure(figsize=(5,4)); plt.plot(fpr, tpr, label=f'ROC AUC={roc_auc:.3f}'); plt.plot([0,1],[0,1],'--', color='gray')\n",
    "plt.xlabel('FPR'); plt.ylabel('TPR'); plt.legend(); plt.title('ROC'); plt.tight_layout(); plt.show()\n",
    "plt.figure(figsize=(5,4)); plt.plot(recall, precision, label=f'PR AUC={pr_auc:.3f}')\n",
    "plt.xlabel('Recall'); plt.ylabel('Precision'); plt.legend(); plt.title('PR'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Probability summary (sanity check)\n",
    "print('Sample probabilities (floored for display):')\n",
    "print(pd.Series(proba_print[:10]).round(6).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980e8f9",
   "metadata": {},
   "source": [
    "## 6. Key Factors for Fraud Prediction\n",
    "\n",
    "Based on coefficients and domain logic, typical strong predictors include:\n",
    "\n",
    "- `is_TRANSFER` and `is_CASH_OUT`: common fraud path is TRANSFER → CASH_OUT\n",
    "\n",
    "- `amt_log` and `is_high_value`: larger transactions are riskier and often flagged\n",
    "\n",
    "- `orig_error`/`dest_error`: accounting inconsistencies indicate abnormal balance transitions\n",
    "\n",
    "- `orig_went_zero`: emptying an account in a single step\n",
    "\n",
    "- Time features (`hour`, `is_weekend`): off-hours may correlate with higher risk\n",
    "\n",
    "- `dest_is_merchant`: merchant destinations have missing balances by design; the contrast can be predictive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a7e2e8",
   "metadata": {},
   "source": [
    "## 7. Interpretation of Factors\n",
    "\n",
    "These factors align with the data dictionary and known fraud patterns:\n",
    "\n",
    "- Fraud agents transfer funds and then cash out; hence high odds for `TRANSFER`/`CASH_OUT`.\n",
    "\n",
    "- Business rule flags >200k explain why `is_high_value` relates to risk; however, the model may capture risk at lower amounts too.\n",
    "\n",
    "- Balance errors (`orig_error`, `dest_error`) arise from mismatches in recorded balances; repeated inconsistencies suggest manipulation or data artifacts.\n",
    "\n",
    "- Time windows reflect operational coverage; higher fraud during low-staff hours is common.\n",
    "\n",
    "If any factor appears counterintuitive, validate via partial dependence/ICE plots or SHAP values in a tree-based model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe0746",
   "metadata": {},
   "source": [
    "## 8. Prevention Recommendations\n",
    "\n",
    "- Real-time scoring with low-latency feature store; auto-block above a dynamic threshold tuned for acceptable false positives.\n",
    "\n",
    "- Step-up authentication for risky patterns (e.g., TRANSFER followed by CASH_OUT within N hours, high-value, off-hours).\n",
    "\n",
    "- Velocity rules: per-origin daily amount/transaction count caps; sudden spikes trigger holds.\n",
    "\n",
    "- Beneficiary reputation: risk scores for destinations; new or rarely used recipients require additional verification.\n",
    "\n",
    "- Device/IP/User behavior analytics; geolocation anomalies; impossible travel checks.\n",
    "\n",
    "- Human-in-the-loop queue with SLA; continuous feedback loop to retrain models weekly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9d0ba",
   "metadata": {},
   "source": [
    "## 9. Evaluation of Actions\n",
    "\n",
    "- Define KPIs: fraud loss per 1k transactions, detection rate (recall), false positive rate, alert volume, customer friction.\n",
    "\n",
    "- A/B or phased rollout: holdout control group without new controls; compare KPIs over same period.\n",
    "\n",
    "- Pre/post analysis with seasonality controls; use difference-in-differences if groups available.\n",
    "\n",
    "- Monitor for drift: population stability index (PSI), feature/score drift; retrain triggers.\n",
    "\n",
    "- Cost-benefit analysis combining model threshold with operational costs (manual review, customer churn risk).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2916b",
   "metadata": {},
   "source": [
    "## Data Dictionary Summary\n",
    "\n",
    "- **step**: Unit of time (1 step = 1 hour, total 744 steps for 30 days)\n",
    "\n",
    "- **type**: Transaction type (CASH-IN, CASH-OUT, DEBIT, PAYMENT, TRANSFER)\n",
    "\n",
    "- **amount**: Transaction amount in local currency\n",
    "\n",
    "- **nameOrig**: Customer initiating the transaction\n",
    "\n",
    "- **oldbalanceOrg**: Initial balance of the originator before the transaction\n",
    "\n",
    "- **newbalanceOrig**: New balance of the originator after the transaction\n",
    "\n",
    "- **nameDest**: Recipient customer\n",
    "\n",
    "- **oldbalanceDest**: Initial balance of the recipient before the transaction (missing for merchants)\n",
    "\n",
    "- **newbalanceDest**: New balance of the recipient after the transaction (missing for merchants)\n",
    "\n",
    "- **isFraud**: Indicates if the transaction is fraudulent (target variable)\n",
    "\n",
    "- **isFlaggedFraud**: Flags illegal attempts (transfers > 200,000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Persist model and quick inference demo\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Save trained pipeline\n",
    "model_path = f\"fraud_lr_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib\"\n",
    "joblib.dump(clf, model_path)\n",
    "print(f\"Saved pipeline to: {model_path}\")\n",
    "\n",
    "# Reload and run a tiny inference demo on a few validation rows\n",
    "pipe = joblib.load(model_path)\n",
    "sample = X_test.head(5).copy()\n",
    "proba_demo = pipe.predict_proba(sample)[:,1]\n",
    "print(\"Sample probabilities:\")\n",
    "print(pd.Series(proba_demo, index=sample.index).round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f01d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. XGBoost baseline + SHAP\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'xgboost'])\n",
    "    import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist',\n",
    "    random_state=RANDOM_STATE,\n",
    "    scale_pos_weight=(y_train.value_counts()[0] / max(1, y_train.value_counts()[1]))\n",
    ")\n",
    "\n",
    "from sklearn.pipeline import Pipeline as SkPipe\n",
    "xgb_clf = SkPipe(steps=[('prep', preprocess), ('model', xgb_model)])\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "proba_xgb = xgb_clf.predict_proba(X_test)[:,1]\n",
    "print('[XGB] ROC AUC:', roc_auc_score(y_test, proba_xgb))\n",
    "print('[XGB] PR  AUC:', average_precision_score(y_test, proba_xgb))\n",
    "\n",
    "# SHAP (robust)\n",
    "try:\n",
    "    import shap\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'shap', '-q'])\n",
    "    import shap\n",
    "\n",
    "# Prepare background and explanation samples\n",
    "preproc = xgb_clf.named_steps.get('prep', preprocess)\n",
    "# Use small samples for SHAP speed; this does not affect evaluation metrics\n",
    "X_bg = X_train.sample(min(2000, len(X_train)), random_state=RANDOM_STATE)\n",
    "X_sh = X_test.sample(min(3000, len(X_test)), random_state=RANDOM_STATE)\n",
    "\n",
    "# Transform to model input space\n",
    "X_bg_trans = preproc.transform(X_bg)\n",
    "X_sh_trans = preproc.transform(X_sh)\n",
    "\n",
    "# Densify if sparse\n",
    "try:\n",
    "    import scipy.sparse as sp\n",
    "    if sp.issparse(X_bg_trans):\n",
    "        X_bg_trans = X_bg_trans.toarray()\n",
    "    if sp.issparse(X_sh_trans):\n",
    "        X_sh_trans = X_sh_trans.toarray()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Feature names, if available\n",
    "feat_names = None\n",
    "try:\n",
    "    feat_names = preproc.get_feature_names_out()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(f\"SHAP background: {X_bg_trans.shape}, explain: {X_sh_trans.shape}\")\n",
    "\n",
    "# Build explainer on the raw XGBoost model, compute SHAP values with additivity disabled for proba\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(xgb_clf.named_steps['model'])\n",
    "    shap_values = explainer.shap_values(X_sh_trans, check_additivity=False)\n",
    "except Exception:\n",
    "    # Fallback: model-agnostic on predict_proba\n",
    "    explainer = shap.Explainer(xgb_clf.named_steps['model'].predict_proba, X_bg_trans)\n",
    "    shap_values = explainer(X_sh_trans)\n",
    "\n",
    "# Beeswarm summary\n",
    "try:\n",
    "    values = shap_values if isinstance(shap_values, (list, tuple)) else getattr(shap_values, 'values', shap_values)\n",
    "    shap.summary_plot(values, X_sh_trans, feature_names=feat_names, max_display=20, show=True)\n",
    "except Exception as e:\n",
    "    print('SHAP summary plot failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21983bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Calibrated probabilities (Logistic)\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# Use the tuned pipeline's preprocessing and create a fresh base model\n",
    "base_lr = LogisticRegression(max_iter=2000, class_weight='balanced', solver='lbfgs', C=clf.named_steps['model'].C)\n",
    "calibrated = Pipeline(steps=[('prep', preprocess), ('model', base_lr)])\n",
    "calibrated = CalibratedClassifierCV(calibrated, method='isotonic', cv=3, n_jobs=-1)\n",
    "\n",
    "calibrated.fit(X_train, y_train)\n",
    "proba_cal = calibrated.predict_proba(X_test)[:,1]\n",
    "print('[Calibrated LR] ROC AUC:', roc_auc_score(y_test, proba_cal))\n",
    "print('[Calibrated LR] PR  AUC:', average_precision_score(y_test, proba_cal))\n",
    "print('[Calibrated LR] Brier:', brier_score_loss(y_test, proba_cal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac559ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Stratified CV report (PR AUC)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "lr_scores = cross_val_score(clf, X, y, scoring='average_precision', cv=cv, n_jobs=-1)\n",
    "print('[LR tuned] PR AUC CV:', lr_scores.round(4), 'mean=', lr_scores.mean().round(4))\n",
    "\n",
    "xgb_scores = cross_val_score(xgb_clf, X, y, scoring='average_precision', cv=cv, n_jobs=-1)\n",
    "print('[XGB] PR AUC CV:', xgb_scores.round(4), 'mean=', xgb_scores.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98175f06",
   "metadata": {},
   "source": [
    "## Model card (concise)\n",
    "- Data: 6.36M transactions (CSV); target: isFraud. Train/validation split: stratified, 30% validation.\n",
    "- Features: domain features (amt_log, is_TRANSFER, is_CASH_OUT, errors, time), categorical OHE, numeric scaled.\n",
    "- Models:\n",
    "  - Logistic Regression (balanced class weights) with tuned C; calibrated probabilities (isotonic).\n",
    "  - XGBoost (hist, imbalance via scale_pos_weight) with SHAP for explainability.\n",
    "- Metrics (validation): report ROC AUC, PR AUC; threshold by F2; confusion matrix.\n",
    "- CV: 3-fold stratified PR AUC for LR (tuned) and XGB.\n",
    "- Risks: class imbalance, potential sampling bias if using sample mode; data drift; threshold needs re-tuning post-deployment.\n",
    "- Ops: export pipeline with joblib; monitor PR AUC, drift (PSI), and alert volumes; retrain cadence weekly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. XGBoost SHAP explanations (auto-install + beeswarm)\n",
    "try:\n",
    "    import shap  # type: ignore\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    print(\"Installing shap...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"shap\", \"-q\"])  # quiet install\n",
    "    import shap  # type: ignore\n",
    "\n",
    "# Validate fitted XGBoost artifacts\n",
    "if 'xgb_model' not in globals():\n",
    "    raise RuntimeError(\"XGBoost model not found. Please run the XGBoost training cell first.\")\n",
    "\n",
    "# Locate a fitted preprocessor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preproc = None\n",
    "if 'xgb_clf' in globals():\n",
    "    # Try common names\n",
    "    for key in (\"preprocess\", \"prep\", \"preproc\"):\n",
    "        if hasattr(xgb_clf, \"named_steps\") and key in xgb_clf.named_steps:\n",
    "            preproc = xgb_clf.named_steps[key]\n",
    "            break\n",
    "    # Try to find CT in steps\n",
    "    if preproc is None and hasattr(xgb_clf, \"steps\"):\n",
    "        for name, step in xgb_clf.steps:\n",
    "            if isinstance(step, ColumnTransformer):\n",
    "                preproc = step\n",
    "                break\n",
    "# Fallback to global variable\n",
    "if preproc is None and 'preprocess' in globals():\n",
    "    preproc = preprocess\n",
    "\n",
    "if preproc is None:\n",
    "    raise RuntimeError(\"Could not find a ColumnTransformer preprocessor associated with XGBoost.\")\n",
    "\n",
    "# Sample for SHAP to keep it fast\n",
    "n_bg = 3000\n",
    "n_sample = 5000\n",
    "X_bg = X_train.sample(n=min(n_bg, len(X_train)), random_state=RANDOM_STATE)\n",
    "X_sh = X_test.sample(n=min(n_sample, len(X_test)), random_state=RANDOM_STATE)\n",
    "\n",
    "# Transform to model space\n",
    "X_bg_t = preproc.transform(X_bg)\n",
    "X_sh_t = preproc.transform(X_sh)\n",
    "\n",
    "# Densify if needed\n",
    "try:\n",
    "    import scipy.sparse as sp\n",
    "    if sp.issparse(X_bg_t):\n",
    "        X_bg_t = X_bg_t.toarray()\n",
    "    if sp.issparse(X_sh_t):\n",
    "        X_sh_t = X_sh_t.toarray()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Get feature names if available\n",
    "feat_names_xgb = None\n",
    "try:\n",
    "    feat_names_xgb = preproc.get_feature_names_out()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(f\"SHAP background: {X_bg_t.shape}, explain: {X_sh_t.shape}\")\n",
    "\n",
    "# Build explainer and plot\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(xgb_model)\n",
    "    shap_values = explainer.shap_values(X_sh_t, check_additivity=False)\n",
    "except Exception:\n",
    "    # Fallback to model-agnostic\n",
    "    explainer = shap.Explainer(xgb_model.predict_proba, X_bg_t)\n",
    "    shap_values = explainer(X_sh_t)\n",
    "\n",
    "# Summary beeswarm\n",
    "try:\n",
    "    shap.summary_plot(\n",
    "        shap_values if isinstance(shap_values, (list, tuple)) else getattr(shap_values, 'values', shap_values),\n",
    "        X_sh_t,\n",
    "        feature_names=feat_names_xgb,\n",
    "        show=True,\n",
    "        max_display=25\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"SHAP summary plot failed:\", e)\n",
    "\n",
    "print(\"Top SHAP features computed and plotted (see chart above).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4856c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. XGBoost evaluation on full test set + operating threshold (F2 and optional cost-based)\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, precision_recall_curve,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "assert 'xgb_clf' in globals(), \"xgb_clf pipeline not found. Train XGBoost cell first.\"\n",
    "\n",
    "# Helper: choose threshold by F2 on a validation split from training (no test leakage)\n",
    "def pick_threshold_f2(estimator, X_tr, y_tr, random_state=RANDOM_STATE):\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    # Take the first fold as validation for speed\n",
    "    train_idx, val_idx = next(skf.split(X_tr, y_tr))\n",
    "    X_tr_sub, y_tr_sub = X_tr.iloc[train_idx], y_tr.iloc[train_idx]\n",
    "    X_val_sub, y_val_sub = X_tr.iloc[val_idx], y_tr.iloc[val_idx]\n",
    "    est = estimator\n",
    "    est.fit(X_tr_sub, y_tr_sub)\n",
    "    p = est.predict_proba(X_val_sub)[:, 1]\n",
    "    prec, rec, thr = precision_recall_curve(y_val_sub, p)\n",
    "    # Exclude last threshold which is nan in alignment with prec/rec arrays\n",
    "    beta = 2.0\n",
    "    f2 = (1 + beta**2) * (prec * rec) / (beta**2 * prec + rec + 1e-12)\n",
    "    best_i = int(np.nanargmax(f2))\n",
    "    best_thr = thr[max(0, min(best_i, len(thr) - 1))]\n",
    "    return float(best_thr)\n",
    "\n",
    "# Optional: choose threshold by costs on the same validation split\n",
    "def pick_threshold_cost(estimator, X_tr, y_tr, cost_fp=1.0, cost_fn=10.0, random_state=RANDOM_STATE):\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    train_idx, val_idx = next(skf.split(X_tr, y_tr))\n",
    "    X_tr_sub, y_tr_sub = X_tr.iloc[train_idx], y_tr.iloc[train_idx]\n",
    "    X_val_sub, y_val_sub = X_tr.iloc[val_idx], y_tr.iloc[val_idx]\n",
    "    est = estimator\n",
    "    est.fit(X_tr_sub, y_tr_sub)\n",
    "    p = est.predict_proba(X_val_sub)[:, 1]\n",
    "    prec, rec, thr = precision_recall_curve(y_val_sub, p)\n",
    "    # Convert thresholds to predictions and compute expected cost\n",
    "    # Note: prec/rec arrays are one element longer than thr; iterate over thr\n",
    "    best_thr, best_cost = 0.5, float('inf')\n",
    "    for t in thr:\n",
    "        y_hat = (p >= t).astype(int)\n",
    "        # Costs: FP incur cost_fp, FN incur cost_fn\n",
    "        fp = np.sum((y_hat == 1) & (y_val_sub.values == 0))\n",
    "        fn = np.sum((y_hat == 0) & (y_val_sub.values == 1))\n",
    "        cost = cost_fp * fp + cost_fn * fn\n",
    "        if cost < best_cost:\n",
    "            best_cost, best_thr = cost, float(t)\n",
    "    return float(best_thr)\n",
    "\n",
    "# 1) Evaluate XGB on full test set (no sampling)\n",
    "proba_xgb_full = xgb_clf.predict_proba(X_test)[:, 1]\n",
    "roc_xgb = roc_auc_score(y_test, proba_xgb_full)\n",
    "pr_xgb = average_precision_score(y_test, proba_xgb_full)\n",
    "print(f\"[XGB] ROC AUC (full test): {roc_xgb:.6f}\")\n",
    "print(f\"[XGB] PR  AUC (full test): {pr_xgb:.6f}\")\n",
    "print(f\"Samples evaluated (X_test): {len(X_test)}; No sampling applied.\")\n",
    "\n",
    "# 2) Pick operating thresholds\n",
    "# F2-based threshold\n",
    "thr_f2 = pick_threshold_f2(xgb_clf, X_train, y_train)\n",
    "# Cost-based threshold (customize costs as needed)\n",
    "cost_fp, cost_fn = 1.0, 10.0\n",
    "thr_cost = pick_threshold_cost(xgb_clf, X_train, y_train, cost_fp=cost_fp, cost_fn=cost_fn)\n",
    "\n",
    "# 3) Report metrics at thresholds on the full test set\n",
    "for label, thr in [(\"F2-opt\", thr_f2), (\"Cost-opt\", thr_cost)]:\n",
    "    y_hat = (proba_xgb_full >= thr).astype(int)\n",
    "    print(f\"\\n[XGB @ {label} threshold={thr:.4f}]\")\n",
    "    print(classification_report(y_test, y_hat, digits=4))\n",
    "    cm = confusion_matrix(y_test, y_hat)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# Persist chosen thresholds for reuse\n",
    "xgb_threshold_f2 = float(thr_f2)\n",
    "xgb_threshold_cost = float(thr_cost)\n",
    "print(f\"Saved thresholds → F2: {xgb_threshold_f2:.4f} | Cost({cost_fp:.1f},{cost_fn:.1f}): {xgb_threshold_cost:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Persist XGBoost pipeline + quick inference demo\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "assert 'xgb_clf' in globals(), \"xgb_clf pipeline not found.\"\n",
    "model_dir = Path(\"models\")\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "model_path_xgb = str(model_dir / \"xgb_pipeline.joblib\")\n",
    "\n",
    "joblib.dump(xgb_clf, model_path_xgb)\n",
    "print(f\"Saved XGBoost pipeline → {model_path_xgb}\")\n",
    "\n",
    "# Quick inference demo\n",
    "xgb_loaded = joblib.load(model_path_xgb)\n",
    "proba_demo_xgb = xgb_loaded.predict_proba(X_test.iloc[:5])[:, 1]\n",
    "# Use F2 threshold by default if defined\n",
    "thr_use = xgb_threshold_f2 if 'xgb_threshold_f2' in globals() else 0.5\n",
    "pred_demo_xgb = (proba_demo_xgb >= thr_use).astype(int)\n",
    "print(\"Demo probabilities:\", np.round(proba_demo_xgb, 6))\n",
    "print(f\"Demo predictions @ thr={thr_use:.3f}:\", pred_demo_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cde396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Isotonic-calibrated XGBoost: fit, evaluate, and persist\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "assert 'xgb_model' in globals(), \"xgb_model not found; train XGBoost first.\"\n",
    "\n",
    "# Build a calibrated pipeline by wrapping the trained xgb_model; reuse the same preprocessor as xgb_clf\n",
    "from sklearn.pipeline import Pipeline as SkPipe\n",
    "calibrated_xgb = SkPipe(steps=[\n",
    "    ('preprocess', xgb_clf.named_steps.get('preprocess', preprocess)),\n",
    "    ('model', CalibratedClassifierCV(estimator=xgb_model, method='isotonic', cv=3))\n",
    "])\n",
    "\n",
    "# Fit on training data\n",
    "calibrated_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on full test set (no sampling)\n",
    "proba_xgb_cal = calibrated_xgb.predict_proba(X_test)[:, 1]\n",
    "roc_cal = roc_auc_score(y_test, proba_xgb_cal)\n",
    "pr_cal = average_precision_score(y_test, proba_xgb_cal)\n",
    "brier_cal = brier_score_loss(y_test, proba_xgb_cal)\n",
    "print(f\"[XGB-Calibrated] ROC AUC (full test): {roc_cal:.6f}\")\n",
    "print(f\"[XGB-Calibrated] PR  AUC (full test): {pr_cal:.6f}\")\n",
    "print(f\"[XGB-Calibrated] Brier: {brier_cal:.6f}\")\n",
    "print(f\"Samples evaluated (X_test): {len(X_test)}; No sampling applied.\")\n",
    "\n",
    "# Persist\n",
    "model_path_xgb_cal = str(model_dir / \"xgb_pipeline_calibrated.joblib\")\n",
    "joblib.dump(calibrated_xgb, model_path_xgb_cal)\n",
    "print(f\"Saved calibrated XGBoost pipeline → {model_path_xgb_cal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42568040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Evaluation coverage audit (no assumed/sampled data)\n",
    "import numpy as np\n",
    "\n",
    "# Asserts for full coverage on test set\n",
    "n_test = len(X_test)\n",
    "assert proba_xgb_full.shape[0] == n_test, \"Mismatch in XGB proba length vs X_test rows\"\n",
    "assert not np.isnan(proba_xgb_full).any(), \"NaNs found in XGB probabilities\"\n",
    "\n",
    "# If calibrated model run, check those too\n",
    "if 'proba_xgb_cal' in globals():\n",
    "    assert proba_xgb_cal.shape[0] == n_test, \"Mismatch in calibrated XGB proba length vs X_test rows\"\n",
    "    assert not np.isnan(proba_xgb_cal).any(), \"NaNs found in calibrated XGB probabilities\"\n",
    "\n",
    "print(f\"All checks passed. Test rows: {n_test}. All probabilities computed with no sampling or NaNs.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
